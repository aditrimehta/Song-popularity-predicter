{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4c56b3c",
   "metadata": {},
   "source": [
    "# Testing all models with cleaned data (imbalanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a82689d",
   "metadata": {},
   "source": [
    "## Imports required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf3cbe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error,r2_score,accuracy_score, confusion_matrix,f1_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.svm import SVR,SVC\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.neighbors import KNeighborsClassifier ,KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor, StackingClassifier\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d89fab",
   "metadata": {},
   "source": [
    "## Regression models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b452f5c0",
   "metadata": {},
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae45a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "X = df.drop(columns=['popularity'])  # Replace 'target_column' with your actual target variable\n",
    "y = df['popularity']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3964fc5",
   "metadata": {},
   "source": [
    "### Linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c4b9620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 401.3028\n",
      "R-squared Score: 0.0179\n",
      "Model Accuracy (with ±10% tolerance): 38.83%\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error & R-squared Score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared Score: {r2:.4f}\")\n",
    "\n",
    "# Tolerance-Based Accuracy Calculation\n",
    "tolerance = 10  # Absolute error margin\n",
    "\n",
    "# Count correct predictions within tolerance\n",
    "correct_predictions = np.sum(np.abs(y_test - y_pred) <= tolerance)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_with_tolerance = (correct_predictions / len(y_test)) * 100\n",
    "\n",
    "print(f\"Model Accuracy (with ±{tolerance}% tolerance): {accuracy_with_tolerance:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1aa9f2",
   "metadata": {},
   "source": [
    "### Polynomial Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "817ce53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 393.4078\n",
      "R-squared Score: 0.0372\n",
      "Model Accuracy (with ±10% tolerance): 39.66%\n"
     ]
    }
   ],
   "source": [
    "# Set polynomial degree\n",
    "degree = 3  # You can experiment with higher degrees\n",
    "\n",
    "# Transform features into polynomial features\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_poly)\n",
    "\n",
    "# Calculate Mean Squared Error & R-squared Score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = model.score(X_test_poly, y_test)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared Score: {r2:.4f}\")\n",
    "\n",
    "# Tolerance-Based Accuracy Calculation\n",
    "tolerance = 10  # Absolute error margin\n",
    "correct_predictions = np.sum(np.abs(y_test - y_pred) <= tolerance)\n",
    "accuracy_with_tolerance = (correct_predictions / len(y_test)) * 100\n",
    "\n",
    "print(f\"Model Accuracy (with ±{tolerance}% tolerance): {accuracy_with_tolerance:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7b7668",
   "metadata": {},
   "source": [
    "### KNN Regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91e2ffed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 466.0415\n",
      "R-squared Score: -0.1406\n",
      "Model Accuracy (with ±10 margin): 36.85%\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model (choose k=5 as a starting point)\n",
    "model = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error & R-squared Score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared Score: {r2:.4f}\")\n",
    "\n",
    "# Tolerance-Based Accuracy Calculation (±10 margin)\n",
    "tolerance = 10  # Absolute error margin\n",
    "\n",
    "# Count correct predictions within tolerance\n",
    "correct_predictions = np.sum(np.abs(y_test - y_pred) <= tolerance)\n",
    "\n",
    "# Calculate tolerance-based accuracy\n",
    "accuracy_with_tolerance = (correct_predictions / len(y_test)) * 100\n",
    "print(f\"Model Accuracy (with ±{tolerance} margin): {accuracy_with_tolerance:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5057f5ad",
   "metadata": {},
   "source": [
    "### Random Forest Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b3c6199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 410.3948\n",
      "R-squared Score: -0.0044\n",
      "Model Accuracy (with ±10 margin): 38.56%\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error & R-squared Score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared Score: {r2:.4f}\")\n",
    "\n",
    "# Tolerance-Based Accuracy Calculation (±10 margin)\n",
    "tolerance = 10  # Absolute error margin\n",
    "\n",
    "# Count correct predictions within tolerance\n",
    "correct_predictions = np.sum(np.abs(y_test - y_pred) <= tolerance)\n",
    "\n",
    "# Calculate tolerance-based accuracy\n",
    "accuracy_with_tolerance = (correct_predictions / len(y_test)) * 100\n",
    "print(f\"Model Accuracy (with ±{tolerance} margin): {accuracy_with_tolerance:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1f0665",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9af503e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 405.9112\n",
      "R-squared Score: 0.0066\n",
      "Model Accuracy (with ±10 margin): 43.08%\n"
     ]
    }
   ],
   "source": [
    "# Standardize features and target\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "# Reshape y to 2D before scaling, then back to 1D\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Initialize the model with RBF kernel (default)\n",
    "model = SVR(kernel='rbf')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train_scaled)\n",
    "y_pred_scaled = model.predict(X_test_scaled)\n",
    "\n",
    "# Convert predictions back to original scale\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Calculate Mean Squared Error & R-squared Score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared Score: {r2:.4f}\")\n",
    "\n",
    "# Tolerance-Based Accuracy Calculation (±10 margin)\n",
    "tolerance = 10  # Absolute error margin\n",
    "\n",
    "# Count correct predictions within tolerance\n",
    "correct_predictions = np.sum(np.abs(y_test - y_pred) <= tolerance)\n",
    "\n",
    "# Calculate tolerance-based accuracy\n",
    "accuracy_with_tolerance = (correct_predictions / len(y_test)) * 100\n",
    "print(f\"Model Accuracy (with ±{tolerance} margin): {accuracy_with_tolerance:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2619366",
   "metadata": {},
   "source": [
    "## Classification models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515407ae",
   "metadata": {},
   "source": [
    "### Logistical Regression (without SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c30121d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Testing Threshold: 50\n",
      "Model Accuracy: 57.65%\n",
      "F1 Score: 0.6594827586206896\n",
      "Confusion Matrix:\n",
      "[[ 497  916]\n",
      " [ 348 1224]]\n",
      "========================================\n",
      "Testing Threshold: 60\n",
      "Model Accuracy: 68.91%\n",
      "F1 Score: 0.0\n",
      "Confusion Matrix:\n",
      "[[2057    0]\n",
      " [ 928    0]]\n",
      "========================================\n",
      "Testing Threshold: 70\n",
      "Model Accuracy: 87.24%\n",
      "F1 Score: 0.0\n",
      "Confusion Matrix:\n",
      "[[2604    0]\n",
      " [ 381    0]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_data.csv\")\n",
    "# Loop over each threshold\n",
    "# Define thresholds to test\n",
    "thresholds = [50, 60, 70]\n",
    "for threshold in thresholds:\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Testing Threshold: {threshold}\")\n",
    "    \n",
    "    # Create a fresh copy of the data each time\n",
    "    data = df.copy()\n",
    "\n",
    "    # Convert popularity to binary based on current threshold\n",
    "    data['popularity'] = (data['popularity'] > threshold).astype(int)\n",
    "\n",
    "    # Define features and target\n",
    "    X = data.drop(columns=['popularity'])\n",
    "    y = data['popularity']\n",
    "\n",
    "    # Train-test split with stratification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Train Logistic Regression model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "    print(f\"Model Accuracy: {accuracy:.2f}%\")\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3634c9c4",
   "metadata": {},
   "source": [
    "### Logistical regression with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa2369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing Threshold: 70\n",
      "Model Accuracy: 84.99%\n",
      "F1 Score: 0.13513513513513514\n",
      "Confusion Matrix:\n",
      "[[2502  102]\n",
      " [ 346   35]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_data.csv\")\n",
    "threshold = 70\n",
    "print(\"=\" * 50)\n",
    "print(f\"Testing Threshold: {threshold}\")\n",
    "\n",
    "# Convert popularity to binary\n",
    "df['popularity'] = (df['popularity'] > threshold).astype(int)\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=['popularity'])\n",
    "y = df['popularity']\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.5)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f\"Model Accuracy: {accuracy:.2f}%\")\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"F1 Score:\", f1)\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec778a64",
   "metadata": {},
   "source": [
    "### KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "042241f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing Threshold: 50\n",
      "KNN Model with k=5\n",
      "Model Accuracy: 55.34%\n",
      "F1 Score: 0.5904761904761905\n",
      "Confusion Matrix:\n",
      "[[691 722]\n",
      " [611 961]]\n",
      "==================================================\n",
      "Testing Threshold: 60\n",
      "KNN Model with k=5\n",
      "Model Accuracy: 63.12%\n",
      "F1 Score: 0.30971786833855797\n",
      "Confusion Matrix:\n",
      "[[1637  420]\n",
      " [ 681  247]]\n",
      "==================================================\n",
      "Testing Threshold: 70\n",
      "KNN Model with k=5\n",
      "Model Accuracy: 84.96%\n",
      "F1 Score: 0.096579476861167\n",
      "Confusion Matrix:\n",
      "[[2512   92]\n",
      " [ 357   24]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_data.csv\")\n",
    "thresholds = [50, 60, 70]\n",
    "\n",
    "# Number of neighbors for KNN\n",
    "k = 5\n",
    "\n",
    "for threshold in thresholds:\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Testing Threshold: {threshold}\")\n",
    "\n",
    "    # Copy the original data\n",
    "    data = df.copy()\n",
    "\n",
    "    # Binarize target based on threshold\n",
    "    data['popularity'] = (data['popularity'] > threshold).astype(int)\n",
    "\n",
    "    # Define features and target\n",
    "    X = data.drop(columns=['popularity'])\n",
    "    y = data['popularity']\n",
    "\n",
    "    # Train-test split with stratification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Train KNN\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "    print(f\"KNN Model with k={k}\")\n",
    "    print(f\"Model Accuracy: {accuracy:.2f}%\")\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(\"F1 Score:\", f1)\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc8677",
   "metadata": {},
   "source": [
    "### RFC without smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03f1a882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing Threshold: 50\n",
      "Model Accuracy: 58.22%\n",
      "F1 Score: 0.627872276932259\n",
      "Confusion Matrix:\n",
      " [[ 686  727]\n",
      " [ 520 1052]]\n",
      "==================================================\n",
      "Testing Threshold: 60\n",
      "Model Accuracy: 67.40%\n",
      "F1 Score: 0.3412322274881517\n",
      "Confusion Matrix:\n",
      " [[1760  297]\n",
      " [ 676  252]]\n",
      "==================================================\n",
      "Testing Threshold: 70\n",
      "Model Accuracy: 83.99%\n",
      "F1 Score: 0.16140350877192983\n",
      "Confusion Matrix:\n",
      " [[2461  143]\n",
      " [ 335   46]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# Thresholds to test\n",
    "thresholds = [50,60, 70]\n",
    "\n",
    "# Loop over thresholds\n",
    "for threshold in thresholds:\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Testing Threshold: {threshold}\")\n",
    "\n",
    "    # Convert target variable: Popularity > threshold → 1, else 0\n",
    "    data = df.copy()\n",
    "    data['popularity'] = (data['popularity'] > threshold).astype(int)\n",
    "\n",
    "    # Define features and target\n",
    "    X = data.drop(columns=['popularity'])\n",
    "    y = data['popularity']\n",
    "\n",
    "    # Split data (80% train, 20% test) with stratification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Feature scaling (optional for Random Forest, but kept for consistency)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Initialize and train Random Forest model\n",
    "    rfc = RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        random_state=42,\n",
    "        class_weight=\"balanced\",\n",
    "        max_depth=20,\n",
    "        min_samples_split=5\n",
    "    )\n",
    "    rfc.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = rfc.predict(X_test_scaled)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "    print(f\"Model Accuracy: {accuracy:.2f}%\")\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(\"F1 Score:\", f1)\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c57dec",
   "metadata": {},
   "source": [
    "### RFC with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1354d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing Threshold: 60\n",
      "Model Accuracy: 66.40%\n",
      "F1 Score: 0.3623649078194533\n",
      "Confusion Matrix:\n",
      " [[1697  360]\n",
      " [ 643  285]]\n",
      "==================================================\n",
      "Testing Threshold: 70\n",
      "Model Accuracy: 74.30%\n",
      "F1 Score: 0.25316455696202533\n",
      "Confusion Matrix:\n",
      " [[2088  516]\n",
      " [ 251  130]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# Thresholds to test\n",
    "thresholds = [60, 70]\n",
    "\n",
    "# Loop over each threshold\n",
    "for threshold in thresholds:\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Testing Threshold: {threshold}\")\n",
    "\n",
    "    # Convert target variable: Popularity > threshold → 1, else 0\n",
    "    data = df.copy()\n",
    "    data['popularity'] = (data['popularity'] > threshold).astype(int)\n",
    "\n",
    "    # Define features and target\n",
    "    X = data.drop(columns=['popularity'])\n",
    "    y = data['popularity']\n",
    "\n",
    "    # Split data (80% train, 20% test) with stratification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Apply SMOTE to training data\n",
    "    smote = SMOTE(sampling_strategy=0.5, random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Initialize and train Random Forest model\n",
    "    rfc = RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        random_state=42,\n",
    "        class_weight=\"balanced\",\n",
    "        max_depth=20,\n",
    "        min_samples_split=5\n",
    "    )\n",
    "    rfc.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = rfc.predict(X_test_scaled)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "    print(f\"Model Accuracy: {accuracy:.2f}%\")\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(\"F1 Score:\", f1)\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e08d0e2",
   "metadata": {},
   "source": [
    "### SVM without SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59f35634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing Threshold: 50\n",
      "SVM Model Accuracy: 59.73%\n",
      "F1 Score: 0.6828496042216359\n",
      "Confusion Matrix:\n",
      " [[ 489  924]\n",
      " [ 278 1294]]\n",
      "==================================================\n",
      "Testing Threshold: 60\n",
      "SVM Model Accuracy: 68.84%\n",
      "F1 Score: 0.0\n",
      "Confusion Matrix:\n",
      " [[2055    2]\n",
      " [ 928    0]]\n",
      "==================================================\n",
      "Testing Threshold: 70\n",
      "SVM Model Accuracy: 87.24%\n",
      "F1 Score: 0.0\n",
      "Confusion Matrix:\n",
      " [[2604    0]\n",
      " [ 381    0]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# Define thresholds to loop over\n",
    "thresholds = [50, 60, 70]\n",
    "\n",
    "# Loop over thresholds\n",
    "for threshold in thresholds:\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Testing Threshold: {threshold}\")\n",
    "\n",
    "    # Convert popularity to binary target\n",
    "    data = df.copy()\n",
    "    data['popularity'] = (data['popularity'] > threshold).astype(int)\n",
    "\n",
    "    # Define features and target\n",
    "    X = data.drop(columns=['popularity'])\n",
    "    y = data['popularity']\n",
    "\n",
    "    # Train-test split with stratification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Initialize and train SVM\n",
    "    svm = SVC(kernel='rbf', C=1, probability=True, random_state=42)\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = svm.predict(X_test_scaled)\n",
    "\n",
    "    # Evaluation\n",
    "    accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "    print(f\"SVM Model Accuracy: {accuracy:.2f}%\")\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(\"F1 Score:\", f1)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d618a5cd",
   "metadata": {},
   "source": [
    "### SVM with smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cc9928d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing Threshold: 60\n",
      "SVM Model Accuracy: 68.81%\n",
      "F1 Score: 0.0\n",
      "Confusion Matrix:\n",
      " [[2054    3]\n",
      " [ 928    0]]\n",
      "==================================================\n",
      "Testing Threshold: 70\n",
      "SVM Model Accuracy: 81.64%\n",
      "F1 Score: 0.20348837209302326\n",
      "Confusion Matrix:\n",
      " [[2367  237]\n",
      " [ 311   70]]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# Thresholds to evaluate\n",
    "thresholds = [60, 70]\n",
    "\n",
    "# Loop over each threshold\n",
    "for threshold in thresholds:\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Testing Threshold: {threshold}\")\n",
    "\n",
    "    # Convert target to binary\n",
    "    data = df.copy()\n",
    "    data['popularity'] = (data['popularity'] > threshold).astype(int)\n",
    "\n",
    "    # Define features and target\n",
    "    X = data.drop(columns=['popularity'])\n",
    "    y = data['popularity']\n",
    "\n",
    "    # Train-test split with stratification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Apply SMOTE to training data\n",
    "    smote = SMOTE(sampling_strategy=0.5, random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Initialize and train SVM model\n",
    "    svm = SVC(kernel='rbf', C=1, probability=True, random_state=42)\n",
    "    svm.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = svm.predict(X_test_scaled)\n",
    "\n",
    "    # Evaluate model\n",
    "    accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "    print(f\"SVM Model Accuracy: {accuracy:.2f}%\")\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(\"F1 Score:\", f1)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5597c4",
   "metadata": {},
   "source": [
    "### XGB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c789f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing Threshold: 50\n",
      "XGBoost Model Accuracy: 55.88%\n",
      "F1 Score: 0.6930785364716849\n",
      "Confusion Matrix:\n",
      " [[ 181 1232]\n",
      " [  85 1487]]\n",
      "==================================================\n",
      "Testing Threshold: 60\n",
      "XGBoost Model Accuracy: 50.35%\n",
      "F1 Score: 0.4889655172413793\n",
      "Confusion Matrix:\n",
      " [[ 794 1263]\n",
      " [ 219  709]]\n",
      "==================================================\n",
      "Testing Threshold: 70\n",
      "XGBoost Model Accuracy: 77.72%\n",
      "F1 Score: 0.2569832402234637\n",
      "Confusion Matrix:\n",
      " [[2205  399]\n",
      " [ 266  115]]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# Thresholds to test\n",
    "thresholds = [50, 60, 70]\n",
    "\n",
    "# Loop through thresholds\n",
    "for threshold in thresholds:\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Testing Threshold: {threshold}\")\n",
    "\n",
    "    # Convert popularity to binary based on threshold\n",
    "    data = df.copy()\n",
    "    data['popularity'] = (data['popularity'] > threshold).astype(int)\n",
    "\n",
    "    # Define features and target\n",
    "    X = data.drop(columns=['popularity'])\n",
    "    y = data['popularity']\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # XGBoost Classifier\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=8,\n",
    "        random_state=42,\n",
    "        scale_pos_weight=5  # You can also set this dynamically if needed\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = xgb.predict(X_test_scaled)\n",
    "\n",
    "    # Evaluation\n",
    "    accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "    print(f\"XGBoost Model Accuracy: {accuracy:.2f}%\")\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d8fdc",
   "metadata": {},
   "source": [
    "### Stacking models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "307e02bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing Threshold: 50\n",
      "Stacked Model Accuracy: 59.06%\n",
      "F1 Score: 0.6555806087936866\n",
      "Confusion Matrix:\n",
      "[[ 600  813]\n",
      " [ 409 1163]]\n",
      "==================================================\n",
      "Testing Threshold: 60\n",
      "Stacked Model Accuracy: 68.51%\n",
      "F1 Score: 0.07480314960629922\n",
      "Confusion Matrix:\n",
      "[[2007   50]\n",
      " [ 890   38]]\n",
      "==================================================\n",
      "Testing Threshold: 70\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 46\u001b[0m\n\u001b[0;32m     39\u001b[0m stacked_model \u001b[38;5;241m=\u001b[39m StackingClassifier(\n\u001b[0;32m     40\u001b[0m     estimators\u001b[38;5;241m=\u001b[39mbase_models,\n\u001b[0;32m     41\u001b[0m     final_estimator\u001b[38;5;241m=\u001b[39mmeta_model,\n\u001b[0;32m     42\u001b[0m     cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m \u001b[43mstacked_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[0;32m     49\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m stacked_model\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)\n",
      "File \u001b[1;32mc:\\Users\\aditr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     66\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[0;32m     69\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\aditr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:717\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[1;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     fit_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[1;32m--> 717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aditr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aditr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:254\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cv, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m cv\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    252\u001b[0m         cv\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState()\n\u001b[1;32m--> 254\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcross_val_predict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_estimators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_method_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m# Only not None or not 'drop' estimators will be used in transform.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# Remove the None from the method as well.\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    272\u001b[0m     meth\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (meth, est) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_, all_estimators)\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    275\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\aditr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aditr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\aditr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\aditr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aditr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\aditr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1246\u001b[0m, in \u001b[0;36mcross_val_predict\u001b[1;34m(estimator, X, y, groups, cv, n_jobs, verbose, params, pre_dispatch, method)\u001b[0m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m-> 1246\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_predict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\n\u001b[0;32m   1257\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1259\u001b[0m inv_test_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(test_indices), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m   1260\u001b[0m inv_test_indices[test_indices] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(test_indices))\n",
      "File \u001b[1;32mc:\\Users\\aditr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aditr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\aditr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\aditr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aditr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1331\u001b[0m, in \u001b[0;36m_fit_and_predict\u001b[1;34m(estimator, X, y, train, test, fit_params, method)\u001b[0m\n\u001b[0;32m   1329\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1331\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1332\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(estimator, method)\n\u001b[0;32m   1333\u001b[0m predictions \u001b[38;5;241m=\u001b[39m func(X_test)\n",
      "File \u001b[1;32mc:\\Users\\aditr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aditr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\svm\\_base.py:257\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    256\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m--> 257\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32mc:\\Users\\aditr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\svm\\_base.py:335\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    321\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[0;32m    325\u001b[0m (\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[1;32m--> 335\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass_weight_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# Thresholds to evaluate\n",
    "thresholds = [50, 60, 70]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Testing Threshold: {threshold}\")\n",
    "\n",
    "    # Prepare data\n",
    "    data = df.copy()\n",
    "    data['popularity'] = (data['popularity'] > threshold).astype(int)\n",
    "\n",
    "    X = data.drop(columns=['popularity'])\n",
    "    y = data['popularity']\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Define base models\n",
    "    base_models = [\n",
    "        ('svm', SVC(kernel='rbf', C=1, probability=True, random_state=42)),\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "        ('rfc', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    ]\n",
    "\n",
    "    # Define meta-model\n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Stacking classifier\n",
    "    stacked_model = StackingClassifier(\n",
    "        estimators=base_models,\n",
    "        final_estimator=meta_model,\n",
    "        cv=5\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    stacked_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = stacked_model.predict(X_test_scaled)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "    print(f\"Stacked Model Accuracy: {accuracy:.2f}%\")\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841b4a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing Threshold: 50\n",
      "Stacked Model Accuracy: 59.33%\n",
      "Confusion Matrix:\n",
      "[[ 568  845]\n",
      " [ 369 1203]]\n",
      "==================================================\n",
      "Testing Threshold: 60\n",
      "Stacked Model Accuracy: 68.84%\n",
      "Confusion Matrix:\n",
      "[[2053    4]\n",
      " [ 926    2]]\n",
      "==================================================\n",
      "Testing Threshold: 70\n",
      "Stacked Model Accuracy: 87.24%\n",
      "Confusion Matrix:\n",
      "[[2604    0]\n",
      " [ 381    0]]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# Thresholds to evaluate\n",
    "thresholds = [50, 60, 70]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Testing Threshold: {threshold}\")\n",
    "\n",
    "    # Prepare data\n",
    "    data = df.copy()\n",
    "    data['popularity'] = (data['popularity'] > threshold).astype(int)\n",
    "\n",
    "    X = data.drop(columns=['popularity'])\n",
    "    y = data['popularity']\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Define base models\n",
    "    base_models = [\n",
    "        ('svm', SVC(kernel='rbf', C=1, probability=True, random_state=42)),\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ]\n",
    "\n",
    "    # Define meta-model\n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Stacking classifier\n",
    "    stacked_model = StackingClassifier(\n",
    "        estimators=base_models,\n",
    "        final_estimator=meta_model,\n",
    "        cv=5\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    stacked_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = stacked_model.predict(X_test_scaled)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "    print(f\"Stacked Model Accuracy: {accuracy:.2f}%\")\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f67ea6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing Threshold: 50\n",
      "Stacked Model Accuracy: 57.99%\n",
      "Confusion Matrix:\n",
      "[[ 605  808]\n",
      " [ 446 1126]]\n",
      "==================================================\n",
      "Testing Threshold: 60\n",
      "Stacked Model Accuracy: 68.54%\n",
      "Confusion Matrix:\n",
      "[[2006   51]\n",
      " [ 888   40]]\n",
      "==================================================\n",
      "Testing Threshold: 70\n",
      "Stacked Model Accuracy: 87.17%\n",
      "Confusion Matrix:\n",
      "[[2595    9]\n",
      " [ 374    7]]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# Thresholds to evaluate\n",
    "thresholds = [50, 60, 70]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Testing Threshold: {threshold}\")\n",
    "\n",
    "    # Prepare data\n",
    "    data = df.copy()\n",
    "    data['popularity'] = (data['popularity'] > threshold).astype(int)\n",
    "\n",
    "    X = data.drop(columns=['popularity'])\n",
    "    y = data['popularity']\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Define base models\n",
    "    base_models = [\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "        ('rfc', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    ]\n",
    "\n",
    "    # Define meta-model\n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Stacking classifier\n",
    "    stacked_model = StackingClassifier(\n",
    "        estimators=base_models,\n",
    "        final_estimator=meta_model,\n",
    "        cv=5\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    stacked_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = stacked_model.predict(X_test_scaled)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "    print(f\"Stacked Model Accuracy: {accuracy:.2f}%\")\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fcc927",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
